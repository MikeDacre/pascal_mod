#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Rewrite of the shell script for easy running from anywhere with cluster support
"""
from __future__ import print_function
import os as os
from os.path import join as _pth
import sys as _sys
from time import sleep as _sleep
import subprocess as _sub
import argparse as _argparse
import multiprocessing as _mp
from random import randint as _randint

# Get rid of pandas future warnings
import warnings as _warn
_warn.simplefilter(action='ignore', category=FutureWarning)
_warn.simplefilter(action='ignore', category=UserWarning)

# Try to use dill
try:
    import dill as _pickle
except ImportError:
    import pickle as _pickle

# For analysis
try:
    import pandas as pd
except ImportError:
    pd = None
try:
    import numpy as _np
except ImportError:
    _np = None
try:
    from scipy import stats as _sts
except ImportError:
    _sts = None
try:
    # github.com/MikeDacre/mike_tools
    _sys.path.append('/home/dacre/code/mike_tools/python')
    import plots as _plots
except ImportError:
    _plots = None

# For progress bar
try:
    from tqdm import tqdm as _tqdm
    from tqdm import tqdm_notebook as _tqnb
    try:
        if str(type(get_ipython())) == "<class 'ipykernel.zmqshell.ZMQInteractiveShell'>":
            _pb = _tqnb
        else:
            _pb = _tqdm
    except NameError:
        _pb = _tqdm
except ImportError:
    _pb = None


try:
    import fyrd as _fyrd
except ImportError:
    _fyrd = None

VERBOSE = False  # Show debug output

###############################################################################
#                           Modify DataFrame Class                            #
###############################################################################


# This is only really useful in the CombinedData DataFrames
def _beats_perm(self, column, mode='lt', cutoff=None):
    """Returns a filtered dataframe of rows where column beats permutations.

    Required that permutation calculations have already been done. Uses the
    column name prefixed with 'perm_' to search for permutations.

    e.g. a column of 'pvalue_open' would look for 'perm_pvalue_open'

    Parameters
    ----------
    column : str
    mode : str
        One of lt (less than) or gt (greater than)
    cutoff : float, optional
        Drop rows that do not beat this number (uses mode to define choice)

    Returns
    -------
    DataFrame
    """
    if mode not in ['lt', 'gt']:
        raise ValueError('Invalid mode: {}'.format({}))
    if not column in self.columns:
        raise ValueError('{} not a column'.format(column))
    perm_col = 'perm_{}'.format(column)
    if not perm_col in self.columns:
        raise ValueError('Permutation column {} does not exist'
                         .format(perm_col))
    if cutoff is None:
        df = self
    else:
        if mode == 'lt':
            df = self[self[column] < cutoff]
        elif mode == 'gt':
            df = self[self[column] > cutoff]
    df = df.dropna(subset=[column, perm_col])
    if mode == 'lt':
        return df[df[column] < df[perm_col]]
    return df[df[column] > df[perm_col]]

pd.DataFrame.beats_perm = _beats_perm

###############################################################################
#                           Main Execution Function                           #
###############################################################################


def run_pascal(sample_file, verbose=False, **kwargs):
    """Run the pascal Java command, please include outdir in kwargs."""
    cmnd = (
        'export DYLD_LIBRARY_PATH={path}/lib:$LD_LIBRARY_PATH; \n'
        'export LD_LIBRARY_PATH="{path}/lib:{path}/lib/openBLASlib/lib:'
        '{path}/lib/fortranlibs:$LD_LIBRARY_PATH"; \n'
        'export OPENBLAS_NUM_THREADS=1; \n'
        'java -ea -Xmx{mem} -jar {path}/jars/pascalDeployed.jar {args}'
    )

    path, java64 = get_runtime_info()
    mem = '32g' if java64 else '2g'
    strtdir = os.path.abspath(os.curdir)
    outdir = kwargs['outdir'] if 'outdir' in kwargs else '.'
    outdir = os.path.abspath(outdir)
    argstr = get_pascal_arg_string(sample_file, **kwargs)

    if not os.path.isdir(outdir):
        os.mkdir(outdir)

    name = os.path.join(
        outdir,
        os.path.splitext(os.path.basename(sample_file))[0]
    )

    try:
        os.chdir(path)
        if verbose:
            print(cmnd.format(path=path, mem=mem, args=argstr))
        print('Running Pascal on {}'.format(sample_file))
        out, err, code = _run(cmnd.format(path=path, mem=mem, args=argstr))
    finally:
        os.chdir(strtdir)

    with open(name + '.out', 'w') as fout:
        fout.write(out)
    with open(name + '.err', 'w') as fout:
        fout.write(err)

    return out, err, code


###############################################################################
#                               Parsing Pascal                                #
###############################################################################


class PascalData(object):

    """DataFrame representations of the outputs from pascal.

    Attributes
    ----------
    genescores : DataFrame
    fgenescores : DataFrame
    pathway : DataFrame
    snperr : DataFrame
    """

    genescores  = pd.DataFrame()
    fgenescores = pd.DataFrame()
    pathway     = pd.DataFrame()
    snperr      = pd.DataFrame()

    def __init__(self, directory, prefix):
        """Initialize DataFrames from a pascal output directory.

        Parameters
        ----------
        directory : str
            The directory where the outputs are located
        prefix : str
            The prefix, which comes from the name of the input file minus the
            file extension
        """
        self.directory = directory
        self.prefix    = prefix
        assert os.path.isdir(directory)
        self.genefile   = None
        self.fgenefile  = None
        self.snperrfile = None
        self.pathfile   = None
        for fl in os.listdir(directory):
            if not os.path.basename(fl).startswith(prefix):
                continue
            fl = os.path.join(directory, fl.strip())
            if fl.endswith('.fusion.genescores.txt'):
                self.fgenefile = fl
            elif fl.endswith('.genescores.txt'):
                self.genefile = fl
            elif fl.endswith('.numSnpError.txt'):
                self.snperrfile = fl
            elif 'PathwaySet' in fl:
                self.pathfile = fl
        missing = []
        if not self.genefile:
            missing.append('genescores')
        if not self.fgenefile:
            missing.append('fgenescores')
        if not self.pathfile:
            missing.append('pathway')
        if not self.snperrfile:
            missing.append('snperr')
        if missing:
            fstr = 'file is' if len(missing) == 1 else 'files are'
            raise OSError(
                'The {} {} missing from the output directory {}.'
                .format(', '.join(missing), fstr, directory)
            )
        self.genescores  = pd.read_csv(self.genefile, sep='\t')
        self.fgenescores = pd.read_csv(self.fgenefile, sep='\t')
        self.pathway     = pd.read_csv(self.pathfile, sep='\t')
        self.snperr      = pd.read_csv(self.snperrfile, sep='\t')

    def __repr__(self):
        return (
            "PascalData<{}:genescores[{}],fgenescores[{}],"
            "pathwayset[{}],snperr[{}]>"
        ).format(
            os.path.join(self.directory, self.prefix),
            len(self.genescores),
            len(self.fgenescores),
            len(self.pathway),
            len(self.snperr)
        )


def run_parse_pascal(sample_file, outdir='.', **pascalargs):
    """Parse a sample file with pascal and return a PascalData object.

    Parameters
    ----------
    sample_file : str
        Path to a sample file (a newline separated list of
    outdir : str
    pascalargs : keyword arguments
        Any of the pascal arguments (to see these, run
        argument_parser(get_pascal_args=True)

    Returns
    -------
    PascalData
    """
    if not pd:
        raise ImportError('cannot parse pascal output without pandas')
    if not pascalargs:
        pascalargs = dict(outdir=outdir)
    else:
        pascalargs.update(dict(outdir=outdir))
    out, err, code = run_pascal(sample_file, **pascalargs)
    if code != 0:
        raise _sub.CalledProcessError(code, 'pascal', output=out, stderr=err)
    return PascalData(outdir, os.path.splitext(sample_file)[0])


###############################################################################
#                            Comparing Two Samples                            #
###############################################################################


class CombinedData(object):

    """Merged DataFrame representations of the outputs from comparative pascal.

    Attributes
    ----------
    sample_1 : PascalData
    sample_2 : PascalData
    genescores : DataFrame
    fgenescores : DataFrame
    pathway : DataFrame
    snperr : DataFrame
    perm_count : int
        A count of the number of permutations parsed successfully
    length_differences : str
        A string describing the length differences between sample and merged
        dataframes

    Methods
    -------
    write(prefix, outdir)
        Write all merged output files to outdir
    """

    sample_1    = PascalData
    sample_2    = PascalData
    genescores  = pd.DataFrame
    fgenescores = pd.DataFrame
    pathway     = pd.DataFrame
    snperr      = pd.DataFrame
    perms_added = False
    diffs_done  = False
    perm_count  = 0

    def __init__(self, directory, mode='new', prefix_1=None, prefix_2=None,
                 suffix_1=None, suffix_2=None, merge_prefix=None):
        """Create merged data from pascal output directory.

        Parameters
        ----------
        directory : str
            The directory where the outputs are located
        mode : str
            One of 'new', 'auto', or 'read'. Use read if comparison has already
            been generated and sample DataFrames are not required.
        prefix_1 : str
            The prefix, which comes from the name of the first input file minus
            the file extension
        prefix_2 : str
            The prefix, which comes from the name of the second input file
            minus the file extension
        suffix_1 : str
            A suffix to use for column names for this sample.
        suffix_2 : str
            A suffix to use for column names for this sample.
        merge_prefix : str
            A prefix to use for the merged files, defaults to the same as the
            directory name
        """
        if mode not in ['new', 'read', 'auto']:
            raise ValueError('Invalid mode: {}'.format(mode))

        self.prefix_1  = prefix_1
        self.prefix_2  = prefix_2
        directory      = os.path.abspath(directory)
        self.directory = directory
        dirname        = os.path.basename(directory)
        self.prefix    = merge_prefix if merge_prefix else dirname

        fls = [
            _pth(directory, self.prefix) + i for i in [
                '.genescores.txt', '.fusion.genescores.txt',
                '.numSnpError.txt', '.PathwaySet.txt'
            ]
        ]
        not_found = []
        for f in fls:
            if not os.path.isfile(f):
                not_found.append(f)

        if mode == 'auto':
            mode = 'new' if not_found else 'read'

        if mode == 'read' and not_found:
            raise OSError('The following files were not found: {}, '
                          .format(not_found) + 'cannot run in read mode')
        if mode == 'new' and (not prefix_1 or not prefix_2):
            raise ValueError('Must provide prefices')

        if mode == 'new':
            self.sample_1 = PascalData(directory, prefix_1)
            self.sample_2 = PascalData(directory, prefix_2)
            self.sample_1_label = prefix_1
            self.sample_2_label = prefix_2
            suffix_1 = suffix_1 if suffix_1 else '{}'.format(prefix_1)
            suffix_2 = suffix_2 if suffix_2 else '{}'.format(prefix_2)

        if not suffix_1.startswith('_'):
            suffix_1 = '_' + suffix_1
        if not suffix_2.startswith('_'):
            suffix_2 = '_' + suffix_2

        self.suffix_1 = suffix_1
        self.suffix_2 = suffix_2

        # Make merged dataframes
        if mode == 'new':
            self.genescores = pd.merge(
                self.sample_1.genescores,
                self.sample_2.genescores,
                on=['chromosome', 'start', 'end', 'strand',
                    'gene_id', 'gene_symbol'],
                how='outer', suffixes=(suffix_1, suffix_2)
            )
            self.fgenescores = pd.merge(
                self.sample_1.fgenescores,
                self.sample_2.fgenescores,
                on=['chromosome', 'start', 'end', 'strand',
                    'gene_id', 'gene_symbol'],
                how='outer', suffixes=(suffix_1, suffix_2)
            )
            self.snperr = pd.merge(
                self.sample_1.snperr,
                self.sample_2.snperr,
                on=['gene_id', 'symbol'],
                how='outer', suffixes=(suffix_1, suffix_2)
            )
            self.pathway = pd.merge(
                self.sample_1.pathway,
                self.sample_2.pathway,
                how='outer', on='Name', suffixes=(suffix_1, suffix_2)
            )
            self.compute_differences()
        else:
            self.genescores  = pd.read_csv(fls[0], sep='\t')
            self.fgenescores = pd.read_csv(fls[1], sep='\t')
            self.snperr      = pd.read_csv(fls[2], sep='\t')
            self.pathway     = pd.read_csv(fls[3], sep='\t')
            if 'pvalue_diff' not in self.genescores.columns:
                self.compute_differences()

    @property
    def length_differences(self):
        """Return a string describing the differences in length."""
        return (
            'Genescores\nsample_1: {}\nsample_2: {}\nmerged: {}\n'
            'Fusion genescores\nsample_1: {}\nsample_2: {}\nmerged: {}\n'
            'Pathway Sets\nsample_1: {}\nsample_2: {}\nmerged: {}\n'
            'numSnpErrors\nsample_1: {}\nsample_2: {}\nmerged: {}\n'
        ).format(
            len(self.sample_1.genescores), len(self.sample_2.genescores),
            len(self.genescores),
            len(self.sample_1.fgenescores), len(self.sample_2.fgenescores),
            len(self.fgenescores),
            len(self.sample_1.pathway), len(self.sample_2.pathway),
            len(self.pathway),
            len(self.sample_1.snperr), len(self.sample_2.snperr),
            len(self.snperr)
        )

    def compute_differences(self):
        """Calculate the differences between pvalues in all dataframes."""
        s1 = self.suffix_1
        s2 = self.suffix_2
        d = self.genescores
        d['pvalue_diff']     = _np.abs(d['pvalue' + s1] - d['pvalue' + s2])
        d['pvalue_log_diff'] = _np.abs(
            _np.log10(d['pvalue' + s1]) - _np.log10(d['pvalue' + s2])
        )
        d['snps_diff'] = _np.abs(d['numSnps' + s1] - d['numSnps' + s2])
        self.genescores = d
        d = self.fgenescores
        d['pvalue_diff']     = _np.abs(d['pvalue' + s1] - d['pvalue' + s2])
        d['pvalue_log_diff'] = _np.abs(
            _np.log10(d['pvalue' + s1]) - _np.log10(d['pvalue' + s2])
        )
        d['snps_diff'] = _np.abs(d['numSnps' + s1] - d['numSnps' + s2])
        self.fgenescores = d
        d = self.pathway
        d['chi2Pvalue_diff'] = _np.abs(
            d['chi2Pvalue' + s1] - d['chi2Pvalue' + s2]
        )
        d['chi2Pvalue_log_diff'] = _np.abs(
            _np.log10(d['chi2Pvalue' + s1]) - _np.log10(d['chi2Pvalue' + s2])
        )
        d['empPvalue_diff'] = _np.abs(
            d['empPvalue' + s1] - d['empPvalue' + s2]
        )
        d['empPvalue_log_diff'] = _np.abs(
            _np.log10(d['empPvalue' + s1]) - _np.log10(d['empPvalue' + s2])
        )
        self.pathway = d

    def write(self, prefix=None, outdir=None):
        """Write all files to outdir.

        Writes merged versions of all data files

        Parameters
        ----------
        prefix : str
            Prefix to use when writing file name, e.g. trait_merged,
            default is <sample_1_file>_<sample_2_file>
        outdir : str
            Provide a path to a different directory, default is same directory
            as the samples.

        Outputs
        -------
        <prefix>.genescores.txt
        <prefix>.fusion.genescores.txt
        <prefix>.PathwaySets.txt
        <prefix>.numSnpErrors.txt
        <prefix>.CombinedData.pickle
        """
        outdir = outdir if outdir else self.directory
        prefix = prefix if prefix else self.prefix
        self.prefix = prefix
        prefix = _pth(outdir, prefix)
        self.genescores.to_csv(prefix + '.genescores.txt', sep='\t',
                               index=False)
        self.fgenescores.to_csv(prefix + '.fusion.genescores.txt', sep='\t',
                                index=False)
        self.pathway.to_csv(prefix + '.PathwaySet.txt', sep='\t', index=False)
        self.snperr.to_csv(prefix + '.numSnpError.txt', sep='\t', index=False)
        if self.diffs_done:
            with open(prefix + '.perm_summary.txt', 'w') as fout:
                fout.write(self.permutation_summary)
        with open(prefix + '.CombinedData.pickle', 'wb') as fout:
            _pickle.dump(self, fout)

    ##################
    #  Permutations  #
    ##################

    def add_perms(self, perm_dir=None, prefix=None, prefix_1=None,
                  prefix_2=None):
        """Create new dataframes of permuted data.

        genescores, fgenescores, and pathway permuted dataframes are added,
        same columns as originals, but with one additional column: 'perm',
        which gives the permutation count

        Parameters
        ----------
        perm_dir : str
            Path to directory with permutations, default is our directory.
        prefix : str
            Prefix for merged perm files, default is our merged prefix

        Returns
        -------
        data : dict
            Dictionary of MWU objects
        """
        if not perm_dir:
            perm_dir = self.directory
        perm_dir = os.path.abspath(perm_dir)
        prefix   = prefix if prefix else self.prefix
        prefix_1 = prefix_1 if prefix_1 else self.prefix_1
        prefix_2 = prefix_2 if prefix_2 else self.prefix_2

        perm_genescores  = []
        perm_fgenescores = []
        perm_pathway     = []
        self.perm_count  = 0
        s1 = self.suffix_1.strip('_')
        s2 = self.suffix_2.strip('_')
        gcols = ['gene_symbol', 'pvalue_diff', 'pvalue_log_diff',
                 'snps_diff', 'pvalue_' + s1, 'pvalue_' + s2, 'perm']
        pcols = ['Name', 'chi2Pvalue_diff', 'chi2Pvalue_log_diff',
                 'empPvalue_diff', 'empPvalue_log_diff',
                 'chi2Pvalue_' + s1, 'chi2Pvalue_' + s2,
                 'empPvalue_' + s1, 'empPvalue_' + s2, 'perm']
        if _pb:
            pbar = _pb(unit=' perms')
        for pdir in [_pth(perm_dir, f) for f in os.listdir(perm_dir)]:
            if not '_perm_' in os.path.basename(pdir):
                continue
            if not os.path.isdir(pdir):
                continue
            try:
                perm = int(pdir.split('_')[-1])
                perm_prefix = '{}_perm_{}'.format(prefix, perm)
            except ValueError:
                continue
            try:
                data = CombinedData(
                    pdir, mode='auto', merge_prefix=perm_prefix,
                    prefix_1='{}_perm_{}'.format(prefix_1, perm),
                    prefix_2='{}_perm_{}'.format(prefix_2, perm),
                    suffix_1=self.suffix_1, suffix_2=self.suffix_2
                )
            except:
                # If one fails, it is no big deal, just move onto the next
                continue
            data.genescores['perm'] = perm
            perm_genescores.append(data.genescores[gcols].rename(
                columns={i: 'perm_' + i for i in gcols[1:-1]}
            ))
            data.fgenescores['perm'] = perm
            perm_fgenescores.append(data.fgenescores[gcols].rename(
                columns={i: 'perm_' + i for i in gcols[1:-1]}
            ))
            data.pathway['perm'] = perm
            perm_pathway.append(data.pathway[pcols].rename(
                columns={i: 'perm_' + i for i in pcols[1:-1]}
            ))
            if _pb:
                pbar.update()
            self.perm_count += 1
        if _pb:
            pbar.close()

        if self.perm_count >= 1:
            self.perm_genescores  = pd.concat(perm_genescores)
            self.perm_fgenescores = pd.concat(perm_fgenescores)
            self.perm_pathway     = pd.concat(perm_pathway)
            self.perms_added = True
        else:
            _sys.stderr.write('No permutations completed yet')

    def compare_to_perms(self):
        """Get mannwhitney U for all pvalue differences to perm.

        Also adds minimum pvalue column to every dataframe.
        """
        if not self.perms_added:
            _sys.stderr.write('No permutations added yet, cannot compare')
            return
        s1 = self.suffix_1.strip('_')
        s2 = self.suffix_2.strip('_')
        gcols = ['perm_pvalue_diff', 'perm_pvalue_log_diff',
                 'perm_snps_diff', 'perm', 'perm_pvalue_' + s1,
                 'perm_pvalue_' + s2]
        pcols = ['perm_chi2Pvalue_diff', 'perm_chi2Pvalue_log_diff',
                 'perm_empPvalue_diff', 'perm_empPvalue_log_diff', 'perm',
                 'perm_chi2Pvalue_' + s1, 'perm_chi2Pvalue_' + s2,
                 'perm_empPvalue_' + s1, 'perm_empPvalue_' + s2]

        self.mannwhitneyu = {
            'genescores': {
                'pvalue_log_diff': _sts.mannwhitneyu(
                    self.genescores.pvalue_log_diff,
                    self.perm_genescores.perm_pvalue_log_diff
                ),
                s1: _sts.mannwhitneyu(
                    self.genescores['pvalue_' + s1],
                    self.perm_genescores['perm_pvalue_' + s1]
                ),
                s2: _sts.mannwhitneyu(
                    self.genescores['pvalue_' + s2],
                    self.perm_genescores['perm_pvalue_' + s2]
                )
            },
            'fgenescores': {
                'pvalue_log_diff': _sts.mannwhitneyu(
                    self.fgenescores.pvalue_log_diff,
                    self.perm_fgenescores.perm_pvalue_log_diff
                ),
                s1: _sts.mannwhitneyu(
                    self.fgenescores['pvalue_' + s1],
                    self.perm_fgenescores['perm_pvalue_' + s1]
                ),
                s2: _sts.mannwhitneyu(
                    self.fgenescores['pvalue_' + s2],
                    self.perm_fgenescores['perm_pvalue_' + s2]
                ),
            },
            'pathway_chi2': {
                'chi2Pvalue_log_diff': _sts.mannwhitneyu(
                    self.pathway.chi2Pvalue_log_diff,
                    self.perm_pathway.perm_chi2Pvalue_log_diff
                ),
                s1: _sts.mannwhitneyu(
                    self.pathway['chi2Pvalue_' + s1],
                    self.perm_pathway['perm_chi2Pvalue_' + s1]
                ),
                s2: _sts.mannwhitneyu(
                    self.pathway['chi2Pvalue_' + s2],
                    self.perm_pathway['perm_chi2Pvalue_' + s2]
                ),
            },
            'pathway_emp': {
                'empPvalue_log_diff': _sts.mannwhitneyu(
                    self.pathway.empPvalue_log_diff,
                    self.perm_pathway.perm_empPvalue_log_diff
                ),
                s1: _sts.mannwhitneyu(
                    self.pathway['empPvalue_' + s1],
                    self.perm_pathway['perm_empPvalue_' + s1]
                ),
                s2: _sts.mannwhitneyu(
                    self.pathway['empPvalue_' + s2],
                    self.perm_pathway['perm_empPvalue_' + s2]
                )
            }
        }
        summary = self.perm_genescores.groupby('gene_symbol').agg(
            {'perm_pvalue_' + s1: _np.min,
             'perm_pvalue_' + s2: _np.min,
             'perm_pvalue_diff': _np.min,
             'perm_pvalue_log_diff': _np.max,
             'perm_snps_diff': _np.max,
             'perm': _np.count_nonzero}
        ).reset_index()
        dcols = []
        for i in gcols:
            if i in self.genescores.columns:
                dcols.append(i)
        if dcols:
            self.genescores.drop(dcols, axis=1, inplace=True)
        self.genescores = pd.merge(
            self.genescores, summary, how='left', on='gene_symbol'
        )
        summary = self.perm_fgenescores.groupby('gene_symbol').agg(
            {'perm_pvalue_' + s1: _np.min,
             'perm_pvalue_' + s2: _np.min,
             'perm_pvalue_diff': _np.min,
             'perm_pvalue_log_diff': _np.max,
             'perm_snps_diff': _np.max,
             'perm': _np.count_nonzero}
        ).reset_index()
        dcols = []
        for i in gcols:
            if i in self.fgenescores.columns:
                dcols.append(i)
        if dcols:
            self.fgenescores.drop(dcols, axis=1, inplace=True)
        self.fgenescores = pd.merge(
            self.fgenescores, summary, how='left', on='gene_symbol'
        )
        summary = self.perm_pathway.groupby('Name').agg(
            {'perm_chi2Pvalue_' + s1: _np.min,
             'perm_chi2Pvalue_' + s2: _np.min,
             'perm_empPvalue_' + s1: _np.min,
             'perm_empPvalue_' + s2: _np.min,
             'perm_chi2Pvalue_diff': _np.min,
             'perm_chi2Pvalue_log_diff': _np.max,
             'perm_empPvalue_diff': _np.min,
             'perm_empPvalue_log_diff': _np.max,
             'perm': _np.count_nonzero}
        ).reset_index()

        dcols = []
        for i in pcols:
            if i in self.pathway.columns:
                dcols.append(i)
        if dcols:
            self.pathway.drop(dcols, axis=1, inplace=True)
        self.pathway = pd.merge(
            self.pathway, summary, how='left', on='Name'
        )
        self.genescores['beats_perm_diff'] = (
            self.genescores.pvalue_log_diff >
            self.genescores.perm_pvalue_log_diff
        )
        self.genescores['beats_perm_' + s1] = (
            self.genescores['pvalue_' + s1] >
            self.genescores['perm_pvalue_' + s1]
        )
        self.genescores['beats_perm_' + s2] = (
            self.genescores['pvalue_' + s2] >
            self.genescores['perm_pvalue_' + s2]
        )
        self.fgenescores['beats_perm_diff'] = (
            self.fgenescores.pvalue_log_diff >
            self.fgenescores.perm_pvalue_log_diff
        )
        self.fgenescores['beats_perm_' + s1] = (
            self.fgenescores['pvalue_' + s1] >
            self.fgenescores['perm_pvalue_' + s1]
        )
        self.fgenescores['beats_perm_' + s2] = (
            self.fgenescores['pvalue_' + s2] >
            self.fgenescores['perm_pvalue_' + s2]
        )
        self.pathway['beats_perm_chi2_diff'] = (
            self.pathway.chi2Pvalue_log_diff >
            self.pathway.perm_chi2Pvalue_log_diff
        )
        self.pathway['beats_perm_chi2_' + s1] = (
            self.pathway['chi2Pvalue_' + s1] >
            self.pathway['perm_chi2Pvalue_' + s1]
        )
        self.pathway['beats_perm_chi2_' + s2] = (
            self.pathway['chi2Pvalue_' + s2] >
            self.pathway['perm_chi2Pvalue_' + s2]
        )
        self.pathway['beats_perm_emp_diff'] = (
            self.pathway.empPvalue_log_diff >
            self.pathway.perm_empPvalue_log_diff
        )
        self.pathway['beats_perm_emp_' + s1] = (
            self.pathway['empPvalue_' + s1] >
            self.pathway['perm_empPvalue_' + s1]
        )
        self.pathway['beats_perm_emp_' + s2] = (
            self.pathway['empPvalue_' + s2] >
            self.pathway['perm_empPvalue_' + s2]
        )
        self.diffs_done = True

    def plot(self, table, modifier=None, xlabel=None, ylabel=None, title=None,
             write_to=None):
        """Plot a -log10 pvalue scatter graph for the given table.

        Permutation information is included if pre-computed.

        Parameters
        ----------
        table : str
            One of 'genescores', 'fgenescores', or 'pathway'
        modifier : str
            If table is 'pathway' one of 'chi2' or 'emp'
        xlabel, ylabel, title : str
            Optional labels for the plot, defaults to existing labels
        write_to : str
            Path to a png, jpg, or pdf file to save the image

        Returns
        -------
        fig, axes
        """
        if not _plots:
            raise ImportError('Need plots to function')
        if not table in ['genescores', 'fgenescores', 'pathway']:
            raise ValueError('invalid table name')
        if table == 'pathway':
            if not modifier or modifier not in ['chi2', 'emp']:
                raise ValueError('invalid modifier')
            pval1 = '{}Pvalue{}'.format(modifier, self.suffix_1)
            pval2 = '{}Pvalue{}'.format(modifier, self.suffix_2)
            if self.diffs_done:
                bp  = 'beats_perm_{}_diff'.format(modifier)
                mwu = self.mannwhitneyu[
                    '{}_{}'.format(table, modifier)][
                    '{}Pvalue_log_diff'.format(modifier)
                    ]
            else:
                bp = None
        else:
            pval1 = 'pvalue{}'.format(self.suffix_1)
            pval2 = 'pvalue{}'.format(self.suffix_2)
            if self.diffs_done:
                bp  = 'beats_perm_diff'
                mwu = self.mannwhitneyu[table]['pvalue_log_diff']
            else:
                bp = None
        df = getattr(self, table)
        if bp:
            df = df.dropna(subset=[pval1, pval2, bp])
        else:
            df = df.dropna(subset=[pval1, pval2])

        xlabel = xlabel if xlabel else self.suffix_1.strip('_')
        ylabel = ylabel if ylabel else self.suffix_2.strip('_')
        title  = title if title else os.path.basename(self.directory)

        kwargs = dict(
            x=df[pval1], y=df[pval2], xlabel=xlabel, ylabel=ylabel,
            pval=0.05, title=title, log_scale='negative',
            reg_details=False
        )

        if self.diffs_done:
            kwargs.update(dict(
                highlight=df[bp],
                highlight_label='Beats {} Permutations'.format(self.perm_count),
                add_text='MWU P: {:.3}'.format(mwu.pvalue)
            ))

        f, a = _plots.scatter(**kwargs)

        if write_to:
            f.savefig(write_to)

        return f, a

    @property
    def permutation_summary(self):
        """Return a string describing the permutation results by length."""
        outar = [
            self.__repr__(), "",
            "Completed a total of {perms} permutations.",
            "",
            "Pathway Analysis:",
            "    empPvalue{suffix_1}:\t{emp1} beat permutations Best: {pemp1:.4e} MWU: {m1:.4e}",
            "    empPvalue{suffix_2}:\t{emp2} beat permutations  Best: {pemp2:.4e} MWU: {m2:.4e}",
            "    empPvalue_log_diff:\t{emp3} beat permutations Best: {pemp3:.3} MWU: {m3:.4e}",
            "    chi2Pvalue{suffix_1}:\t{chi1} beat permutations Best: {pchi1:.4e} MWU: {m4:.4e}",
            "    chi2Pvalue{suffix_2}:\t{chi2} beat permutations Best: {pchi2:.4e} MWU: {m5:.4e}",
            "    chi2Pvalue_log_diff:\t{chi3} beat permutations Best: {pchi3:.3} MWU: {m6:.4e}",
            "",
            "Genescores:",
            "    pvalue{suffix_1}:\t{gs1} beat permutations Best: {pgs1:.4e} MWU: {m7:.4e}",
            "    pvalue{suffix_2}:\t{gs2} beat permutations Best: {pgs2:.4e} MWU: {m8:.4e}",
            "    pvalue_log_diff:\t{gs3} beat permutations Best: {pgs3:.3} MWU: {m9:.4e}",
            "",
            "Fusion Genescores:",
            "    pvalue{suffix_1}:\t{fs1} beat permutations Best: {pfs1:.4e} MWU: {m10:.4e}",
            "    pvalue{suffix_2}:\t{fs2} beat permutations Best: {pfs2:.4e} MWU: {m11:.4e}",
            "    pvalue_log_diff:\t{fs3} beat permutations Best: {pfs3:.3} MWU: {m12:.4e}",
        ]
        return '\n'.join(outar).format(
            perms=self.perm_count,
            suffix_1=self.suffix_1, suffix_2=self.suffix_2,
            emp1=len(self.pathway.beats_perm('empPvalue' + self.suffix_1)),
            emp2=len(self.pathway.beats_perm('empPvalue' + self.suffix_2)),
            emp3=len(self.pathway.beats_perm('empPvalue_log_diff', 'gt')),
            chi1=len(self.pathway.beats_perm('chi2Pvalue' + self.suffix_1)),
            chi2=len(self.pathway.beats_perm('chi2Pvalue' + self.suffix_2)),
            chi3=len(self.pathway.beats_perm('chi2Pvalue_log_diff', 'gt')),
            gs1=len(self.genescores.beats_perm('pvalue' + self.suffix_1)),
            gs2=len(self.genescores.beats_perm('pvalue' + self.suffix_2)),
            gs3=len(self.genescores.beats_perm('pvalue_log_diff', 'gt')),
            fs1=len(self.fgenescores.beats_perm('pvalue' + self.suffix_1)),
            fs2=len(self.fgenescores.beats_perm('pvalue' + self.suffix_2)),
            fs3=len(self.fgenescores.beats_perm('pvalue_log_diff', 'gt')),
            pemp1=self.pathway.beats_perm('empPvalue' + self.suffix_1)['empPvalue' + self.suffix_1].min(),
            pemp2=self.pathway.beats_perm('empPvalue' + self.suffix_2)['empPvalue' + self.suffix_2].min(),
            pemp3=self.pathway.beats_perm('empPvalue_log_diff', 'gt')['empPvalue_log_diff'].max(),
            pchi1=self.pathway.beats_perm('chi2Pvalue' + self.suffix_1)['chi2Pvalue' + self.suffix_1].min(),
            pchi2=self.pathway.beats_perm('chi2Pvalue' + self.suffix_2)['chi2Pvalue' + self.suffix_2].min(),
            pchi3=self.pathway.beats_perm('chi2Pvalue_log_diff', 'gt')['chi2Pvalue_log_diff'].max(),
            pgs1=self.genescores.beats_perm('pvalue' + self.suffix_1)['pvalue' + self.suffix_1].min(),
            pgs2=self.genescores.beats_perm('pvalue' + self.suffix_2)['pvalue' + self.suffix_2].min(),
            pgs3=self.genescores.beats_perm('pvalue_log_diff', 'gt')['pvalue_log_diff'].max(),
            pfs1=self.fgenescores.beats_perm('pvalue' + self.suffix_1)['pvalue' + self.suffix_1].min(),
            pfs2=self.fgenescores.beats_perm('pvalue' + self.suffix_2)['pvalue' + self.suffix_2].min(),
            pfs3=self.fgenescores.beats_perm('pvalue_log_diff', 'gt')['pvalue_log_diff'].max(),
            m1=self.mannwhitneyu['pathway_emp'][self.suffix_1.strip('_')].pvalue,
            m2=self.mannwhitneyu['pathway_emp'][self.suffix_2.strip('_')].pvalue,
            m3=self.mannwhitneyu['pathway_emp']['empPvalue_log_diff'].pvalue,
            m4=self.mannwhitneyu['pathway_chi2'][self.suffix_1.strip('_')].pvalue,
            m5=self.mannwhitneyu['pathway_chi2'][self.suffix_2.strip('_')].pvalue,
            m6=self.mannwhitneyu['pathway_chi2']['chi2Pvalue_log_diff'].pvalue,
            m7=self.mannwhitneyu['genescores'][self.suffix_1.strip('_')].pvalue,
            m8=self.mannwhitneyu['genescores'][self.suffix_2.strip('_')].pvalue,
            m9=self.mannwhitneyu['genescores']['pvalue_log_diff'].pvalue,
            m10=self.mannwhitneyu['fgenescores'][self.suffix_1.strip('_')].pvalue,
            m11=self.mannwhitneyu['fgenescores'][self.suffix_2.strip('_')].pvalue,
            m12=self.mannwhitneyu['fgenescores']['pvalue_log_diff'].pvalue,
        )

    def __repr__(self):
        """Simple description of lengths of DFs."""
        return (
            "CombinedData<{}:genescores[{}],fgenescores[{}],"
            "pathwayset[{}],snperr[{}]>"
        ).format(
            os.path.join(self.directory, self.prefix),
            len(self.genescores),
            len(self.fgenescores),
            len(self.pathway),
            len(self.snperr)
        )

    def __str__(self):
        """Display perm info if available, else simple lengths."""
        if self.diffs_done:
            return self.permutation_summary
        return repr(self)

    def __len__(self):
        """Alias for perm_count."""
        return self.perm_count



def run_pascal_comp(sample_1, sample_2, outdir='.', merge_prefix=None,
                    sample_1_label='sample_1', sample_2_label='sample_2',
                    parallel=True, verbose=False, **pascalargs):
    """Run pascal twice and compare the outputs.

    Runs pascal twice in parallel with multiprocessing.

    Parameters
    ----------
    sample_1 : str
        File name or path to file with rsids and pvals for sample 1
    sample_2 : str
        File name or path to file with rsids and pvals for sample 2
    outdir : str
        Directory to write the outputs to
    merge_prefix : str
        Prefix to use when writing merged file names, e.g. trait_merged,
        default is <sample_1_file>_<sample_2_file>
    sample_1_label : str
        Label used for columns for this sample (e.g. pvalue becomes
        pvalue_sample_1)
    sample_2_label : str
        Label used for columns for this sample
    parallel : bool
    verbose : bool
    pascalargs : keywords
        Any arguments for pascal (see all with
        argument_parser(get_pascal_args=True))

    Outputs
    -------
    All pascal outputs plus:
    <outdir>/<merge_prefix>.genescores.txt
    <outdir>/<merge_prefix>.fusion.genescores.txt
    <outdir>/<merge_prefix>.PathwaySets.txt
    <outdir>/<merge_prefix>.numSnpErrors.txt

    Raises
    ------
    _sub.CalledProcessError
        If either pascal job fails

    Returns
    -------
    CombinedData
        A CombinedData object containing all merged and sample DataFrames
    """
    for fl in [sample_1, sample_2]:
        bad = []
        if not os.path.isfile(fl):
            bad.append(fl)
        if bad:
            outstr = 'file' if len(bad) == 1 else 'files:'
            raise OSError('Cannot find {} {}'.format(outstr, bad))
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    outdir = os.path.abspath(outdir)
    print(outdir)
    pascalargs.update(dict(outdir=outdir))

    # Run jobs
    if parallel:
        with _mp.Pool(2) as pool:
            job1 = pool.apply_async(run_pascal, (sample_1,), kwds=pascalargs)
            job2 = pool.apply_async(run_pascal, (sample_2,), kwds=pascalargs)
            out1, err1, code1 = job1.get()
            out2, err2, code2 = job2.get()
    else:
        out1, err1, code1 = run_pascal(sample_1, **pascalargs)
        out2, err2, code2 = run_pascal(sample_2, **pascalargs)

    # Check run status
    failed_1 = False
    failed_2 = False
    if code1 != 0:
        _sys.stderr.write('Pascal {} job failed.\nSTDOUT:\n{}STDERR:{}\n'
                          .format(sample_1, out1, err1))
        failed_1 = True
    if code2 != 0:
        _sys.stderr.write('Pascal {} job failed.\nSTDOUT:\n{}STDERR:{}\n'
                          .format(sample_2, out2, err2))
        failed_2 = True
    if failed_1 and failed_2:
        raise _sub.CalledProcessError(code1, 'both pascal jobs')
    elif failed_1:
        raise _sub.CalledProcessError(code1, 'pascal {} job'
                                      .format(sample_1))
    elif failed_2:
        raise _sub.CalledProcessError(code2, 'pascal {} job'
                                      .format(sample_2))

    # Parse Outputs
    prefix_1 = os.path.basename(os.path.splitext(sample_1)[0])
    prefix_2 = os.path.basename(os.path.splitext(sample_2)[0])
    data = CombinedData(outdir, prefix_1=prefix_1, prefix_2=prefix_2,
                        suffix_1=sample_1_label, suffix_2=sample_2_label,
                        merge_prefix=merge_prefix)
    if verbose:
        _sys.stderr.write('{} v {} lengths\n'.format(sample_1, sample_2))
        _sys.stderr.write(data.length_differences)
    data.write(prefix=merge_prefix, outdir=outdir)

    return data


def plot_comparisons(data, folder=None, tables=None):
    """Write png plots to a folder.

    Parameters
    ----------
    data : CombinedData
    folder : str
        Directory to write to defaults to same directory as data
    tables : list
        A list of any of ['genescores', 'fgenescores', 'pathway_chi2'
        'pathway_emp'] to plot, defaults to all.
    """
    folder = os.path.abspath(folder) if folder else data.directory
    assert os.path.isdir(folder)
    default_tables = [
        'genescores', 'fgenescores', 'pathway_chi2', 'pathway_emp'
    ]
    tables = tables if tables else default_tables
    if isinstance(tables, str):
        tables = [tables]
    for t in tables:
        if t not in default_tables:
            raise ValueError('Invalid table {}'.format(t))

    prefix = _pth(folder, data.prefix)
    name = os.path.basename(data.directory)
    if 'genescores' in tables:
        data.plot(
            'genescores', title=name + ' Genescores',
            write_to='{}.genescores.png'.format(prefix)
        )
    if 'fgenescores' in tables:
        data.plot(
            'fgenescores', title=name + ' Fusion Genescores',
            write_to='{}.fusion.genescores.png'.format(prefix)
        )
    if 'pathway_chi2' in tables:
        data.plot(
            'pathway', modifier='chi2',
            title=name + ' Pathway Chi2',
            write_to='{}.PathwaySet.chi2.png'.format(prefix)
        )
    if 'pathway_emp' in tables:
        data.plot(
            'pathway', modifier='emp',
            title=name + ' Pathway emp',
            write_to='{}.PathwaySet.emp.png'.format(prefix)
        )


###############################################################################
#                                Permutations                                 #
###############################################################################


def run_permutation(sample_1, sample_2, outdir, perms=1000,
                    sample_1_label='sample_1', sample_2_label='sample_2',
                    mem_per_job='10000', wait=True, start_perms=None,
                    separate_background=False, fyrd_args=None, **pascalargs):
    """Permute data from two sample sets and run pascal on each combination.

    This function uses fyrd to submit cluster jobs.

    Method: Combine all data from sample_1 and sample_2 into a single list,
    then permute that list with numpy and split is into two new lists of the
    same lengths as sample_1 and sample_2. Then run depict as normal on those
    two files.

    Parameters
    ----------
    sample_1 : str
        File name or path to file with rsids and pvals for sample 1
    sample_2 : str
        File name or path to file with rsids and pvals for sample 2
    perms : int
        Number of permutations.
    mem_per_job : str
        Amount of memory to request for each job
    wait : bool
        Wait for jobs to finish before returning
    start_perms : int
        Start permutation count from this number
    separate_background : bool
        Treat background SNPs—those with a Pval of 1.0—separately.
        Specifically, read them from the files and do not permute them, instead
        keep them as is.
    fyrd_args : dict
        Fyrd keyword arguments, not required.
    pascalargs : keywords
        Any arguments for pascal (see all with
        argument_parser(get_pascal_args=True))
    """
    if not _np:
        raise ImportError('Cannot run permutations without numpy')
    if not pd:
        raise ImportError('Cannot run comparison without pandas')
    if not _fyrd:
        _sys.stderr.write('Fyrd is not installed, running all permutations '
                          'locally\n')
        raise ImportError('Local run not enabled.')

    # Read files into two lists and combine into one 'super' list
    def get_background(snp_list):
        foreground = []
        background = []
        for snp in snp_list:
            _, p = snp.split('\t')
            if p == 1.0:
                background.append(snp)
            else:
                foreground.append(snp)
        return foreground, background
    s1_rsids = []
    s2_rsids = []
    with open(sample_1) as fin:
        s1_rsids += fin.read().strip().split('\n')
    with open(sample_2) as fin:
        s2_rsids += fin.read().strip().split('\n')
    if separate_background:
        s1_rsids, s1_background = get_background(s1_rsids)
        s2_rsids, s2_background = get_background(s2_rsids)

    rsids = _np.array(s1_rsids + s2_rsids)

    # Make sure a minimum reasonable walltime is set
    if not isinstance(fyrd_args, dict):
        fyrd_args = dict()
    if 'walltime' not in fyrd_args and 'time' not in fyrd_args:
        fyrd_args.update(time='00:15:00')

    # Prep for permutations
    outdir    = os.path.abspath(outdir)
    perm_path = _pth(outdir, 'perm_files')
    job_path  = _pth(outdir, 'jobs')
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    if not os.path.isdir(perm_path):
        os.mkdir(perm_path)
    if not os.path.isdir(job_path):
        os.mkdir(job_path)
    #  if not _fyrd:
        #  pool = _mp.Pool()

    jobs  = {}
    if start_perms:
        perm = int(start_perms)
    else:
        done_perms = []
        for fl in os.listdir(outdir):
            if fl.startswith(os.path.basename(outdir + '_perm_')):
                try:
                    done_perms.append(int(fl.split('_')[-1]))
                except ValueError:
                    raise
        perm = max(done_perms)+1 if done_perms else 1

    print('Running {} permutations starting at {}'.format(perms, perm))
    ttl = perms
    if _pb:
        pbar = _pb(total=ttl, unit='perms')
    while perms:
        # Permute data
        this_perm = _np.random.permutation(rsids)
        new_sample_1_data = list(this_perm[:len(s1_rsids)])
        new_sample_2_data = list(this_perm[len(s1_rsids):])
        assert len(new_sample_1_data) == len(s1_rsids)
        assert len(new_sample_2_data) == len(s2_rsids)
        if separate_background:
            new_sample_1_data += s1_background
            new_sample_2_data += s2_background

        # Create the permutation files
        new_sample_1 = _pth(
            os.path.abspath(perm_path),
            '{}_perm_{}.txt'.format(
                os.path.splitext(os.path.basename(sample_1))[0], perm
            )
        )
        new_sample_2 = _pth(
            os.path.abspath(perm_path),
            '{}_perm_{}.txt'.format(
                os.path.splitext(os.path.basename(sample_2))[0], perm
            )
        )
        with open(new_sample_1, 'w') as fout:
            fout.write('\n'.join(new_sample_1_data))
        with open(new_sample_2, 'w') as fout:
            fout.write('\n'.join(new_sample_2_data))

        name       = '{}_perm_{}'.format(os.path.basename(outdir), perm)
        new_outdir = _pth(outdir, name)

        # Build job arguments
        kwargs  = dict(sample_1       = new_sample_1,
                       sample_2       = new_sample_2,
                       outdir         = new_outdir,
                       merge_prefix   = name,
                       sample_1_label = sample_1_label,
                       sample_2_label = sample_2_label)
        if pascalargs:
            kwargs.update(pascalargs)

        # Actually submit the job
        jobs[name] = (
            _fyrd.submit(
                run_pascal_comp,
                kwargs     = kwargs,
                name       = name,
                syspaths   = os.path.split(os.path.realpath(__file__))[0],
                imports    = ['import os as os', 'import sys as _sys',
                              'from os.path import join as _pth',
                              'import subprocess as _sub',
                              'import multiprocessing as _mp',
                              'import pascal', 'from pascal import *',
                              'from pascal import _run',
                              'from pascal import PascalData',
                              'from pascal import CombinedData',
                              'from pascal import get_pascal_arg_string',
                              'from pascal import get_runtime_info',
                              'from pascal import run_pascal',],
                cores      = 2,
                mem        = mem_per_job,
                scriptpath = job_path,
                outpath    = job_path,
                **fyrd_args
            )
        )
        perms -= 1
        perm  += 1
        if _pb:
            pbar.update()
    if _pb:
        pbar.close()

    if not wait:
        return

    # Get output file information
    print('Permutation jobs submitted, waiting for results.')
    outputs = {}
    with _pb(total=ttl, unit='results') as pbar:
        while len(outputs) < len(jobs):
            for name, job in jobs.items():
                if name in outputs:
                    continue
                job.update()
                if job.done:
                    outs = job.get()
                    outputs[name] = outs
                    pbar.update()
            _sleep(1)

    print('Permutation jobs completed.')

    return outputs


def analyse_comparison(sample_1, sample_2, outdir, perms=1000,
                       sample_1_label='sample_1', sample_2_label='sample_2',
                       mem_per_job='10000', start_perms=None,
                       separate_background=False, fyrd_args=None,
                       **pascalargs):
    """Run the complete analysis pipeline.

    Will run:
        - initial comparison
        - permutations
        - permutation parsing
        - plotting

    Parameters
    ----------
    sample_1 : str
        File name or path to file with rsids and pvals for sample 1
    sample_2 : str
        File name or path to file with rsids and pvals for sample 2
    perms : int
        Number of permutations.
    mem_per_job : str
        Amount of memory to request for each job
    start_perms : int
        Start permutation count from this number
    separate_background : bool
        Treat background SNPs—those with a Pval of 1.0—separately.
        Specifically, read them from the files and do not permute them, instead
        keep them as is.
    fyrd_args : dict
        Fyrd keyword arguments, not required.
    pascalargs : keywords
        Any arguments for pascal (see all with
        argument_parser(get_pascal_args=True))

    Returns
    -------
    CombinedData
    """
    if not _np:
        raise ImportError('Cannot run permutations without numpy')
    if not pd:
        raise ImportError('Cannot run comparison without pandas')
    if not _fyrd:
        _sys.stderr.write('Fyrd is not installed, running all permutations '
                          'locally\n')
        raise ImportError('Local run not enabled.')

    # Make sure a minimum reasonable walltime is set
    if not isinstance(fyrd_args, dict):
        fyrd_args = dict()
    if 'walltime' not in fyrd_args and 'time' not in fyrd_args:
        fyrd_args.update(time='00:15:00')

    outdir   = os.path.abspath(outdir)
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    job_path = _pth(outdir, 'jobs')
    if not os.path.isdir(job_path):
        os.makedirs(job_path)

    print('Submitting main comparison to cluster.')
    main_job = _fyrd.submit(
        run_pascal_comp,
        kwargs=dict(
            sample_1=sample_1, sample_2=sample_2, outdir=outdir,
            sample_1_label=sample_1_label, sample_2_label=sample_2_label,
            parallel=True, **pascalargs
        ),
        syspaths   = os.path.split(os.path.realpath(__file__))[0],
        imports    = ['import os as os', 'import sys as _sys',
                      'from os.path import join as _pth',
                      'import subprocess as _sub',
                      'import multiprocessing as _mp',
                      'import pascal', 'from pascal import *',
                      'from pascal import _run',
                      'from pascal import PascalData',
                      'from pascal import CombinedData',
                      'from pascal import get_pascal_arg_string',
                      'from pascal import get_runtime_info',
                      'from pascal import run_pascal',],
        cores      = 2,
        mem        = mem_per_job,
        scriptpath = job_path,
        outpath    = job_path,
        **fyrd_args
    )

    # Will wait for permutations
    if 'wait' in pascalargs:
        pascalargs.pop('wait')
    run_permutation(sample_1, sample_2, outdir, perms=perms,
                    sample_1_label=sample_1_label,
                    sample_2_label=sample_2_label,
                    mem_per_job=mem_per_job, wait=True,
                    start_perms=start_perms,
                    separate_background=separate_background,
                    fyrd_args=fyrd_args, **pascalargs)

    # Get main job
    print('Getting main job')
    data = main_job.get()

    # Analyze permutations
    print('Analyzing permutations')
    data.add_perms()
    data.compare_to_perms()
    print('Writing')
    data.write()

    # Plot
    print('Plotting')
    plot_comparisons(data)

    print('Done\n')
    print(data.permutation_summary)

    return data


def get_datafile(datafile=None, directory=None, prefix=None):
    """Load a CombinedData pickled object.

    Requires either the datafile, or the path and the prefix
    """
    if not datafile:
        if not directory or not prefix:
            raise ValueError('Must provide directory and prefix if datafile ' +
                             'is not given')
        datafile = os.path.abspath(_pth(directory, prefix) +
                                    '.CombinedData.pickle')
    if not os.path.isfile(datafile):
        raise OSError('File not found: {}'.format(datafile))
    with open(datafile, 'rb') as fin:
        data = _pickle.load(fin)
    return data


def get_completed_data(directory, prefix_1=None, prefix_2=None,
                       merge_prefix=None, add_perms=True, perm_dir=None,
                       suffix_1=None, suffix_2=None, print_summary=False):
    """Build data object from pre-completed comparison and permutations.

    Parameters
    ----------
    directory : str
        The directory where the outputs are located
    prefix_1 : str
        The prefix, which comes from the name of the first input file minus
        the file extension
    prefix_2 : str
        The prefix, which comes from the name of the second input file
        minus the file extension
    merge_prefix : str, optional
        A prefix to use for the merged files, defaults to the same as the
        directory name
    add_perms : bool, optional
        Add permutations to the CombinedData, default True
    perm_dir : str, optional
        Only provide if the permutation directory is different
    suffix_1 : str, optional
        A suffix to use for column names for this sample.
    suffix_2 : str, optional
        A suffix to use for column names for this sample.
    print_summary : bool
        Print a summary of pvalues that have beaten permutations if done,
        else print a simple string showing lengths of various dataframes.

    Returns
    -------
    CombinedData
    """
    if merge_prefix:
        pickle_file = _pth(directory, merge_prefix) + '.CombinedData.pickle'
        if not os.path.isfile(pickle_file):
            if not prefix_1 or not prefix_2:
                raise ValueError('Must provide prefix_1 and prefix_2 if no '
                                 'pickled data file')
    else:
        if not prefix_1 or not prefix_2:
            raise ValueError('Must provide prefix_1 and prefix_2 if no '
                             'merge_prefix')
    print('Getting Data')
    if merge_prefix and os.path.isfile(pickle_file):
        with open(pickle_file, 'rb') as fin:
            data = _pickle.load(fin)
    else:
        data = CombinedData(directory, mode='auto', merge_prefix=merge_prefix,
                            prefix_1=prefix_1, prefix_2=prefix_2,
                            suffix_1=suffix_1, suffix_2=suffix_2)
    if add_perms:
        print('Getting Permutations')
        data.add_perms(perm_dir=perm_dir, prefix=merge_prefix)
        print('{} permutations found'.format(data.perm_count))
    if not data.perm_count:
        return data
    data.compare_to_perms()
    if print_summary:
        print(data.permutation_summary)
    return data


def create_table_of_studies(study_info, suffix_1='sample_1',
                            suffix_2='sample_2', outfile=None,
                            add_perms=False):
    """Create a DataFrame of study information for multiple runs.

    All studies must have been previously parsed

    Parameters
    ----------
    study_info : dict
        Dictionary of {prefix: directory} for all studies.
        Can also be a list. List will be turned into a dictionary. If a list
        item contains ':' it will be split on that character for the dictionary
        otherwise the dirctionary key and value will be the same (i.e. the
        prefix and the directory will be identical). This is commonly the case.
    suffix_1, suffix_2 : str, optional
        Suffices for the two samples
    outfile : str, optional
        If provided, write a summary table to this file
    add_perms : bool, optional
        Add additional permutations to each study from permutations in their
        root directory. Default False.

    Return
    ------
    DataFrame
        rows are the prefices, columns:
            perms_done,
            empPvalue_<suffix_1>, empPvalue_<suffix_2>, empPvalue_log_diff,
            chi2Pvalue_<suffix_1>, chi2Pvalue_<suffix_2>, chi2Pvalue_log_diff,
            genescores_pvalue_<suffix_1>, genescores_pvalue_<suffix_2>,
            genescores_pvalue_log_diff, fgenescores_pvalue_<suffix_1>,
            fgenescores_pvalue_<suffix_2>, fgenescores_pvalue_log_diff,
            empPvalue_<suffix_1>_best, empPvalue_<suffix_2>_best, empPvalue_log_diff_best,
            chi2Pvalue_<suffix_1>_best, chi2Pvalue_<suffix_2>_best, chi2Pvalue_log_diff_best,
            genescores_pvalue_<suffix_1>_best, genescores_pvalue_<suffix_2>_best,
            genescores_pvalue_log_diff_best, fgenescores_pvalue_<suffix_1>_best,
            fgenescores_pvalue_<suffix_2>_best, fgenescores_pvalue_log_diff_best
    """
    if isinstance(study_info, (list, tuple, set)):
        s = {}
        for i in study_info:
            if ':' in i:
                k, v = i.split(':')
                s[k] = v
            else:
                s[i] = i
        study_info = s
    columns = [
        'perms_done',
        "empPvalue_" + suffix_1, "empPvalue_" + suffix_2, "empPvalue_log_diff",
        "chi2Pvalue_" + suffix_1, "chi2Pvalue_" + suffix_2, "chi2Pvalue_log_diff",
        "genescores_pvalue_" + suffix_1, "genescores_pvalue_" + suffix_2,
        "genescores_pvalue_log_diff", "fgenescores_pvalue_" + suffix_1,
        "fgenescores_pvalue_" + suffix_2, "fgenescores_pvalue_log_diff",
        "empPvalue_" + suffix_1 + "_best", "empPvalue_" + suffix_2 + "_best", "empPvalue_log_diff" + "_best",
        "chi2Pvalue_" + suffix_1 + "_best", "chi2Pvalue_" + suffix_2 + "_best", "chi2Pvalue_log_diff" + "_best",
        "genescores_pvalue_" + suffix_1 + "_best", "genescores_pvalue_" + suffix_2 + "_best",
        "genescores_pvalue_log_diff" + "_best", "fgenescores_pvalue_" + suffix_1 + "_best",
        "fgenescores_pvalue_" + suffix_2 + "_best", "fgenescores_pvalue_log_diff_best"
    ]
    index = []
    rows = []
    for prefix, directory in study_info.items():
        print('Getting {}'.format(prefix))
        try:
            data = get_completed_data(
                directory, merge_prefix=prefix, add_perms=add_perms
            )
        except ValueError as e:
            if str(e) == (
                "Must provide prefix_1 and prefix_2 if no pickled data file"
            ):
                _sys.stderr.write(
                    '\nCannot build tables without pickled files. '
                    'Parse the output data before running this function.\n'
                    'Affected directory: {}\n\n'.format(directory)
                )
                continue
            else:
                continue
        except:
            _sys.stderr.write('Could not parse {}, skipping'.format(prefix))
            continue
        index.append(prefix)
        rows.append([
            data.perm_count,
            len(data.pathway.beats_perm('empPvalue' + data.suffix_1)),
            len(data.pathway.beats_perm('empPvalue' + data.suffix_2)),
            len(data.pathway.beats_perm('empPvalue_log_diff', 'gt')),
            len(data.pathway.beats_perm('chi2Pvalue' + data.suffix_1)),
            len(data.pathway.beats_perm('chi2Pvalue' + data.suffix_2)),
            len(data.pathway.beats_perm('chi2Pvalue_log_diff', 'gt')),
            len(data.genescores.beats_perm('pvalue' + data.suffix_1)),
            len(data.genescores.beats_perm('pvalue' + data.suffix_2)),
            len(data.genescores.beats_perm('pvalue_log_diff', 'gt')),
            len(data.fgenescores.beats_perm('pvalue' + data.suffix_1)),
            len(data.fgenescores.beats_perm('pvalue' + data.suffix_2)),
            len(data.fgenescores.beats_perm('pvalue_log_diff', 'gt')),
            data.pathway.beats_perm('empPvalue' + data.suffix_1)['empPvalue' + data.suffix_1].min(),
            data.pathway.beats_perm('empPvalue' + data.suffix_2)['empPvalue' + data.suffix_2].min(),
            data.pathway.beats_perm('empPvalue_log_diff', 'gt')['empPvalue_log_diff'].max(),
            data.pathway.beats_perm('chi2Pvalue' + data.suffix_1)['chi2Pvalue' + data.suffix_1].min(),
            data.pathway.beats_perm('chi2Pvalue' + data.suffix_2)['chi2Pvalue' + data.suffix_2].min(),
            data.pathway.beats_perm('chi2Pvalue_log_diff', 'gt')['chi2Pvalue_log_diff'].max(),
            data.genescores.beats_perm('pvalue' + data.suffix_1)['pvalue' + data.suffix_1].min(),
            data.genescores.beats_perm('pvalue' + data.suffix_2)['pvalue' + data.suffix_2].min(),
            data.genescores.beats_perm('pvalue_log_diff', 'gt')['pvalue_log_diff'].max(),
            data.fgenescores.beats_perm('pvalue' + data.suffix_1)['pvalue' + data.suffix_1].min(),
            data.fgenescores.beats_perm('pvalue' + data.suffix_2)['pvalue' + data.suffix_2].min(),
            data.fgenescores.beats_perm('pvalue_log_diff', 'gt')['pvalue_log_diff'].max(),
        ])
    df = pd.DataFrame(rows, columns=columns, index=index)
    if outfile:
        if outfile.endswith('xls') or outfile.endswith('xlsx'):
            df.to_excel(outfile)
        else:
            df.to_csv(outfile, sep='\t')
    return df


###############################################################################
#                              Helper Functions                               #
###############################################################################


def get_pascal_arg_string(sample_file, **kwargs):
    """Take all keyword arguments and return an argument string for pascal."""
    allowed_args = ['chr', 'up', 'down', 'maxsnp', 'outsuffix', 'custom',
                    'mergeddistance', 'mafcutoff', 'genesetfile', 'customdir',
                    'set', 'randseed']
    argstr = '--pval={} '.format(os.path.abspath(sample_file))

    # Handle outdir parsing
    if 'outdir' in kwargs:
        #  outdir = os.path.basename(kwargs.pop('outdir'))
        outdir = os.path.abspath(kwargs.pop('outdir'))
        argstr += '--outdir={} '.format(outdir)

    # Settings file
    if 'settings' in kwargs:
        settings = kwargs.pop('settings')
        kwargs.update(dict(set=settings))

    # Gene Scoring Method
    if 'genescoring' in kwargs:
        scoremethod = kwargs.pop('genescoring')
    else:
        scoremethod = 'sum'
    scoremethod = scoremethod.lower()
    if scoremethod not in ['max', 'sum']:
        raise ValueError('Invalid genescoring method: {}. Choose max or sum'
                         .format(scoremethod))
    argstr += '--maxvegas ' if scoremethod == 'max' else '--analyticvegas '

    # Calculate pathway scores
    if 'runpathway' in kwargs:
        v = kwargs.pop('runpathway')
        m = kwargs.pop('mergedistance') if 'mergedistance' in kwargs else None
        if v:
            if not m:
                m = 1
            argstr += '--runpathway --mergedistance={}'.format(m)

    # Handle simple arguments
    for k, v in kwargs.items():
        if not v:
            continue
        if k in allowed_args:
            argstr += '--{}={} '.format(k, v)
        else:
            _sys.stderr.write('Ignoring unrecognized argument --{}\n'
                              .format(k))
    return argstr


def create_pascal_infile(sample, file_name):
    """Create an input file for pascal.

    Parameters
    ----------
    sample : list_or_DataFrame
        A list of rsIDs and pvalues either as a two column DataFrame or a
        list of tuples.
    file_name : str
        Valid path to non-existing file with write permission

    Returns
    -------
    path : str
    """
    if not pd:
        raise ImportError('cannot parse pascal output without pandas')
    if not isinstance(sample, pd.core.frame.DataFrame):
        sample = list(sample)
        if not isinstance(sample[0], tuple):
            raise ValueError(
                'Sample must be a 2 column DataFrame or list of tuples'
            )
        sample = pd.DataFrame(sample)
        sample = sample[[sample.columns[0], sample.columns[1]]]
    sample.to_csv(file_name, header=False, index=False)
    return file_name


def get_runtime_info():
    """Return a information sufficient to run pascal.

    Returns
    -------
    path : string
        absolute path to pascal's root
    java64 : bool
        True if 64 bit java
    """
    path = os.path.dirname(os.path.realpath(__file__))
    if 'jars' not in os.listdir(path):
        path = os.path.dirname(path)
        if 'jars' not in os.listdir(path):
            raise OSError('Cannot find PATH to Pascal, make sure this script' +
                          ' is in the root or bin directory of the Pascal' +
                          ' package.')
    assert os.path.isdir(path)
    assert os.path.isdir(os.path.join(path, 'lib'))
    assert os.path.isdir(os.path.join(path, 'jars'))

    _, err, code = _run('java -version')
    if int(code) != 0 or not err:
        raise OSError('Java does not appear to be installed as ' +
                      '`java --version` fails')
    if '64-Bit' in err:
        java64 = True
    else:
        java64 = False
        _sys.stderr.write(
            "java 32-Bit called instead of 64-Bit. You might run into "
            "memory problems.\nConsider installing the 64-Bit java VM.\n"
        )

    return path, java64


def _run(command, raise_on_error=False):
    """Run a command with subprocess the way it should be.

    Parameters
    ----------
    command : str
        A command to execute, piping is fine.
    raise_on_error : bool
        Raise a subprocess.CalledProcessError on exit_code != 0

    Returns
    -------
    stdout : str
    stderr : str
    exit_code : int
    """
    pp = _sub.Popen(command, shell=True, universal_newlines=True,
                    stdout=_sub.PIPE, stderr=_sub.PIPE)
    out, err = pp.communicate()
    code = pp.returncode
    if raise_on_error and code != 0:
        raise _sub.CalledProcessError(
            returncode=code, cmd=command, output=out, stderr=err
        )
    return out, err, code


###############################################################################
#                            Command Line Parsing                             #
###############################################################################


def argument_parser(get_pascal_args=False):
    """Build an argument parser for command line running.

    get_pascal_args prompts the return of a formatted string of pascal options.
    """
    path, _ = get_runtime_info()
    parser = _argparse.ArgumentParser(
        description=__doc__,
        formatter_class=_argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Show debug output')

    ###################################
    #  Shared parser for outdir only  #
    ###################################

    shared_outdir = _argparse.ArgumentParser(add_help=False)
    shared_outdir.add_argument('outdir', help='Directory to output files to')

    ############################################
    #  Shared parser for all pascal arguments  #
    ############################################

    shared_parent = _argparse.ArgumentParser(add_help=False)
    shared_parent.add_argument('--outsuffix',
                               help=(
                                   'Adds an additional string to the output file '
                                   'names produced.'
                               ))

    shared = shared_parent.add_argument_group(
        'pascal runtime arguments'
    )
    shared.add_argument('--settings',
                        default = os.path.join(
                            path, 'settings.txt'
                        ), help='Settings file with default pascal settings.')
    shared.add_argument('--randseed', type=int, default=_randint(1, 999999999),
                        help='set a random seed')
    shared.add_argument('--chr', type=int, help='limit to this chromosome')
    shared.add_argument('--up', type=int, default=50000,
                        help=(
                            'Gives the number of base-pairs upstream of the '
                            'transcription start site that are still counted '
                            'as belonging to the gene region. The default is '
                            "50’000."
                        ))
    shared.add_argument('--down', type=int, default=50000,
                        help=(
                            'Gives the number of base-pairs downstream of '
                            'the gene-body that are still counted as '
                            'belonging to the gene region. The default is '
                            "50’000."
                        ))
    shared.add_argument('--maxsnp', type=int, default=3000,
                        help=(
                            'Sets the aximum number of SNPs per gene. If a '
                            'gene has more SNPs in its region it will not '
                            'calculate the score. If the option is set to -1, '
                            'all genes will be computed. The default is 3000.'
                        ))
    shared.add_argument('--genescoring', type=str, choices={'sum', 'max'},
                        default='sum', help=(
                            'Chooses the genescoring method. The default is '
                            'sum. This option should be supplied with either'
                            'max or sum.'
                        ))
    shared.add_argument('--runpathway', action='store_true',
                        help=(
                            'Chooses whether Pascal should be calculate '
                            'pathway scores.'
                        ))
    shared.add_argument('--mergedistance', type=int, default=1,
                        help=(
                            'Gives the genomic distance in mega-bases that '
                            'the program uses to fuse nearby genes during the '
                            'pathway analysis. Only has an effect if '
                            'runpathway is on. The default is 1.'
                        ))
    shared.add_argument('--mafcutoff', type=float, default=0.05,
                        help=(
                            'SNPs with maf below that value in the european '
                            'sample of 1KG will be ignored.  The default is '
                            '0.05'
                        ))
    shared.add_argument('--genesetfile',
                        default = os.path.join(
                            path, 'resources/genesets/msigdb/' +
                            'msigBIOCARTA_KEGG_REACTOME.gmt'
                        ),
                        help=(
                            'Gives the file name to a gmt-file where the gene '
                            'sets are defined. The default is '
                            'resources/genesets/msigdb/'
                            'msigBIOCARTA_KEGG_REACTOME.gmt'
                        ))

    cref = shared_parent.add_argument_group(
        'custom reference',
        description=(
            'Pascal allows to use custom reference populations instead of the '
            '1KG-EUR sam- ple (For an example about formatting and parameter '
            'setting, check the bash script examples/exampleRunFromTped.sh). '
            'To make use of this option, you have to provide your genotype '
            'information in gnu-zipped, space-separated and 1-2-coded '
            'tped-files split by chromosome. the tped format has been '
            'popularized by the plink-tool for GWAS analysis.'
        )
    )
    cref.add_argument('--customdir',
                      help=(
                          'Gives the path to where the gnu-zipped tped-files '
                          'are stored.'
                      ))
    cref.add_argument('--custom',
                      help=(
                          'Gives the prefix of a gnuzipped tped-files. The '
                          'total filename per chromosome is '
                          '<prefix>.chr<chrNr>.tped.gz.'
                      ))

    # Return just the pascal args if desired
    if get_pascal_args:
        return shared_parent.format_help()

    ###########################################
    #  Shared Parser for All Comparison Runs  #
    ###########################################

    shared_comp = _argparse.ArgumentParser(add_help=False)
    shared_comp.add_argument('sample_1', help='File of rsid\\tpval to analyze')
    shared_comp.add_argument('sample_2', help='File of rsid\\tpval to analyze')
    shared_comp.add_argument('--s1-label', default='sample_1',
                             help='A label to use for sample_1')
    shared_comp.add_argument('--s2-label', default='sample_2',
                             help='A label to use for sample_2')

    #######################################
    #  Shared Parser For Cluster Running  #
    #######################################

    shared_cluster = _argparse.ArgumentParser(add_help=False)
    permo = shared_cluster.add_argument_group(
        'permutation options',
        description="Options for generating permutations."
    )
    permo.add_argument('--perms', default=1000, type=int,
                       help='Number of permutations to run')
    permo.add_argument('--start-at', type=int,
                       help='Start from permutation #')
    permo.add_argument('--background', action='store_true',
                       help='When running permutations, maintain background ' +
                       'as constant, only permute SNPs with a pvalue. To do ' +
                       'this, your list must contain background SNPs with a ' +
                       'p-value set at exactly 1.0. These SNPs will not be ' +
                       'permuted, but will be the same in every permutation.')
    cluster = shared_cluster.add_argument_group(
        'cluster running options',
        description="Options for cluster jobs when running with fyrd."
    )
    cluster.add_argument('--walltime', default='00:20:00',
                         help='Amount of time to request per permutation ' +
                         '(cluster running only)')
    cluster.add_argument('--queue',
                         help='Queue to run jobs in (cluster running only)')
    cluster.add_argument('--profile',
                         help='Fyrd profile to use (cluster running only)')
    cluster.add_argument('--keep-job-files', action='store_false',
                         help='Do not delete job files (cluster running only)')
    cluster.add_argument('--no-wait', action='store_false',
                         help='Do not wait for permutations to finish')
    cluster.add_argument('--mem', default='8G',
                         help='Amount of memory to request per permutation')


    ##################################################
    #  Define Run modes, independent of pascal args  #
    ##################################################

    modes = parser.add_subparsers(title='mode', dest='mode')

    # Run simply with only one sample
    single = modes.add_parser(
        'single', parents=[shared_outdir, shared_parent],
        help="Run pascal directly on a single dataset"
    )
    single.add_argument('snp_file', help='File of rsid\\tpval to analyze')

    # Run a simple comparison
    double = modes.add_parser(
        'double', parents=[shared_outdir, shared_comp, shared_parent],
        help="Run pascal on two datasets and created merge comparison files." +
        " Runs in parallel with two cores."
    )
    double.add_argument('--prefix',
                        help="A prefix to use for merged file names.")

    # Run permutations only
    modes.add_parser(
        'permute',
        parents=[shared_outdir, shared_comp, shared_cluster, shared_parent],
        help="Run pascal permutations, randomizes contents of two samples " +
        "and runs pascal on each. Defaults to use a cluster with the fyrd " +
        "module. Each permutation should take around 2 minutes and 8G of " +
        "memory. If fyrd is not installed, local threads are used."
    )

    # Run permutations only
    modes.add_parser(
        'analyze',
        parents=[shared_outdir, shared_comp, shared_cluster, shared_parent],
        help="Runs a complete analysis on the cluster, including the main " +
        "analysis, permutations, and plotting. Plotting is done locally."
    )

    # Write Updated DFs with permutation info
    parse = modes.add_parser(
        'parse', parents=[shared_outdir, shared_comp],
        help="Parse a directory containing combined data and permutations " +
        "and create new tables with integrated permutation information."
    )
    parse.add_argument('--prefix',
                       help="A prefix to use for merged files (defaults to " +
                       "directory name). Sometimes makes parsing faster.")
    parse.add_argument('--plot', action='store_true',
                       help="Also write plot files for all tables")
    parse.add_argument('--show-summary', action='store_true',
                       help="Also print a summary of the permutations")
    parse.add_argument('--write-summary',
                       help="Also write the summary to this file")
    parse.add_argument('--no-perms', action='store_false',
                       help="Do not parse permutations")

    # Write Plots Only
    plot = modes.add_parser(
        'plot', parents=[shared_outdir, shared_comp],
        help="Create scatter plots of exiting data."
    )
    plot.add_argument('--prefix',
                      help="A prefix to use for merged files (defaults to " +
                      "directory name). Sometimes makes parsing faster.")
    plot.add_argument('--no-perms', action='store_false',
                      help="Do not parse permutations")

    # Write a summary table
    summary = modes.add_parser(
        'summary', help='Write a summary table of study info'
    )
    summary.add_argument('outfile', help='file to write to')
    summary.add_argument('study_info',
                         help='study info in the format:\n ' +
                         'prefix:directory,[prefix:directory,...]')
    summary.add_argument('--suffix-1', default='sample_1',
                         help='suffix used for sample 1')
    summary.add_argument('--suffix-2', default='sample_2',
                         help='suffix used for sample 2')
    summary.add_argument('--add-perms', action='store_true',
                         help='Also add new permutations to data')


    # Settings dump
    settings = modes.add_parser(
        'dump-settings',help='Dump a settings file for manual configuration' +
        '. Use with --settings.'
    )
    settings.add_argument('-o', dest='outfile', default='settings.txt',
                          help='Path to output file for settings, default ' +
                          'settings.txt')

    return parser


###############################################################################
#                                Run Directly                                 #
###############################################################################


def main(argv=None):
    """Run as a script."""
    if not argv:
        argv = _sys.argv[1:]

    parser = argument_parser()

    args = parser.parse_args(argv)

    if not args.mode:
        parser.print_help()
        _sys.exit(1)

    if args.verbose:
        global VERBOSE
        VERBOSE = True

    # Convert to dictionary
    opts = vars(args)
    if VERBOSE:
        print(opts)

    if args.mode == 'dump-settings':
        path, _ = get_runtime_info()
        with open(_pth(path, 'settings.txt')) as fin, open(args.outfile, 'w') as fout:
            fout.write(fin.read())
        return 0
    if args.mode == 'summary':
        study_info = {}
        for i in args.study_info.split(','):
            if ':' in i:
                k, v = i.split(':')
                study_info[k] = v
            else:
                study_info[i] = i
        create_table_of_studies(
            study_info, suffix_1=args.suffix_1, suffix_2=args.suffix_2,
            outfile=args.outfile, add_perms=args.add_perms
        )
        return 0

    outdir = args.outdir
    outdir = os.path.abspath(outdir if outdir.strip() else os.curdir)
    mode = opts.pop('mode')
    if mode == 'single':
        sample_file = opts.pop('snp_file')
        code = run_pascal(sample_file, **opts)[3]
        return code

    sample_1 = opts.pop('sample_1')
    sample_2 = opts.pop('sample_2')
    sample_1_label = opts.pop('s1_label')
    sample_2_label = opts.pop('s2_label')

    if mode == 'double':
        merge_prefix = opts.pop('prefix')
        if not merge_prefix:
            merge_prefix = os.path.basename(outdir)
        run_pascal_comp(sample_1, sample_2,
                        merge_prefix=merge_prefix,
                        sample_1_label=sample_1_label,
                        sample_2_label=sample_2_label,
                        **opts)
    elif mode == 'permute':
        wait = opts.pop('no_wait')
        perms = opts.pop('perms')
        startat = opts.pop('start_at')
        mem = opts.pop('mem')
        time = opts.pop('walltime')
        queue = opts.pop('queue')
        profile = opts.pop('profile')
        background = opts.pop('background')
        fyrd_args = dict(time=time)
        if queue:
            fyrd_args.update(dict(partition=queue))
        if profile:
            fyrd_args.update(dict(profile=profile))
        fyrd_args.update(dict(clean_files=False, clean_outputs=False))
        opts.pop('outdir')
        run_permutation(sample_1, sample_2, outdir, perms=perms,
                        sample_1_label=sample_1_label,
                        sample_2_label=sample_2_label,
                        mem_per_job=mem, wait=wait,
                        start_perms=startat, separate_background=background,
                        fyrd_args=fyrd_args, **opts)
    elif mode == 'analyze':
        wait = opts.pop('no_wait')
        perms = opts.pop('perms')
        startat = opts.pop('start_at')
        mem = opts.pop('mem')
        time = opts.pop('walltime')
        queue = opts.pop('queue')
        profile = opts.pop('profile')
        background = opts.pop('background')
        fyrd_args = dict(time=time)
        if queue:
            fyrd_args.update(dict(partition=queue))
        if profile:
            fyrd_args.update(dict(profile=profile))
        fyrd_args.update(dict(clean_files=False, clean_outputs=False))
        opts.pop('outdir')
        analyse_comparison(sample_1, sample_2, outdir, perms=perms,
                           sample_1_label=sample_1_label,
                           sample_2_label=sample_2_label,
                           mem_per_job=mem, wait=wait,
                           start_perms=startat, separate_background=background,
                           fyrd_args=fyrd_args, **opts)
    elif mode == 'parse' or mode == 'plot':
        prefix_1 = os.path.splitext(os.path.basename(sample_1))[0]
        prefix_2 = os.path.splitext(os.path.basename(sample_2))[0]
        data = get_completed_data(
            outdir, prefix_1=prefix_1, prefix_2=prefix_2,
            add_perms=args.no_perms, suffix_1=sample_1_label,
            suffix_2=sample_2_label, merge_prefix=args.prefix
        )
        if mode == 'parse':
            print('Writing updated tables')
            data.write()
        if mode == 'plot' or args.plot:
            print('Plotting data')
            plot_comparisons(data)
        if mode == 'parse' and args.show_summary:
            print('Summary:\n')
            print(data.permutation_summary)
        if mode == 'parse' and args.write_summary:
            with open(args.write_summary, 'w') as fout:
                fout.write(data.permutation_summary + '\n')

    else:
        parser.print_help()
        return 1

    return 0

if __name__ == '__main__' and '__file__' in globals():
    _sys.exit(main())
